{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Открой файл [distances.ipynb](src/notebooks/distances.ipynb). \n",
    "* Объедини общие данные о фильмах [tmdb_5000_movies](https://files.sberdisk.ru/s/te4QbzdxKgsFQXA) и каст фильмов \n",
    "[tmdb_5000_credits](https://files.sberdisk.ru/s/H9oRuXQt5mFz3T9). \n",
    "* Оставь в датасете только фильмы, которые вышли в \"релиз\".\n",
    "* Убери фильмы с пропусками в колонках ['overview', 'genres', 'keywords']\n",
    "* Выведи количество фильмов, оставшихся в выборке "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество фильмов оставшихся в выборке: 4792\n"
     ]
    }
   ],
   "source": [
    "# загрузка данных из файлов\n",
    "movies_dataset = pd.read_csv('../../datasets/tmdb_5000_movies.csv')\n",
    "credits_dataset = pd.read_csv('../../datasets/tmdb_5000_credits.csv')\n",
    "\n",
    "# объединение данных с указанием суффиксов для дублирующихся столбцов\n",
    "dataset = movies_dataset.merge(credits_dataset, left_on='id', right_on='movie_id', suffixes=('_1', '_2'))\n",
    "\n",
    "# фильтрация по релизу\n",
    "dataset = dataset[dataset['status'] == 'Released']\n",
    "\n",
    "# удаление фильмов с пропусками в столбцах 'overview', 'genres', 'keywords'\n",
    "dataset = dataset.dropna(subset=['overview', 'genres', 'keywords'])\n",
    "\n",
    "# удаление дублирующихся столбцов 'movie_id' и 'title_y'\n",
    "dataset.drop(columns=['movie_id', 'title_2'], inplace=True)\n",
    "\n",
    "# переименование столбца 'title_1' в 'title'\n",
    "dataset.rename(columns={'title_1': 'title'}, inplace=True)\n",
    "\n",
    "print(\"Количество фильмов оставшихся в выборке:\", len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуем алгоритм рекомендации на основе описания фильма (`overview`) и ключевых слов к фильму (`keywords`). \n",
    "Объедини тексты этих колонок и проведи предобработку:\n",
    "* Замени NaN в описании фильма на пустой символ `''`\n",
    "* Удали все английские стоп-слова (используй параметр `stop_words` в `TfidfVectorizer`)\n",
    "* Рассчитай матрицу [Tf-Idf](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) для описания фильмов.\n",
    "\n",
    "Выведи размер получившейся матрицы\n",
    "> Параметр `max_features` в `TfidfVectorizer` должен быть равен 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "размер матрицы Tf-Idf: (4792, 10000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# замена NaN в описании фильма на пустой символ\n",
    "dataset['overview'].fillna('')\n",
    "\n",
    "# создание экземпляра TfidfVectorizer с удалением английских стоп-слов\n",
    "vectorizer = TfidfVectorizer(stop_words='english',  max_features=10000)\n",
    "\n",
    "# расчёт Tf-Idf для описания фильма\n",
    "tfidf_matrix = vectorizer.fit_transform(dataset['overview'] + ' ' + dataset['keywords'])\n",
    "\n",
    "print(\"размер матрицы Tf-Idf:\", tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассчитай [cosine similarity](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html) \n",
    "между фильмами. Составь из этой матрицы `pd.DataFrame`. Для дальнейшего удобства, \n",
    "колонки и индексы таблицы назови согласно`id` фильма. \\\n",
    "Сохрани получившийся `DataFrame` c расстояниями в папку [assets](src/assets) с названием `distance.csv`.\n",
    "А сам объединенный датасет с фильмами сохрани в папку [assets](src/assets) с названием `movies.csv`.\n",
    "\n",
    "> Получившиеся файлы `distance.csv` и `movies.csv` пушить в GitLab не нужно!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# рассчет cosine similarity между фильмами\n",
    "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# создание DataFrame с расстояниями\n",
    "movies_ids = dataset['id']\n",
    "cosine_sim_df = pd.DataFrame(cosine_sim, index=movies_ids, columns=movies_ids)\n",
    "\n",
    "# сохранение DataFrame с расстояниями в файл\n",
    "cosine_sim_df.to_csv('../assets/distance.csv')\n",
    "\n",
    "# сохранение объединенного датасета с фильмами в файл\n",
    "dataset.to_csv('../assets/movies.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
